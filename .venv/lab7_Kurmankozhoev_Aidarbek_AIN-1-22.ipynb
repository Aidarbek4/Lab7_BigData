{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:22:43.886266Z",
     "start_time": "2024-12-11T16:20:02.356880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Для работы со стоп-словами и токенизацией\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "id": "582e03238ce768c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aidar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aidar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T16:22:43.976536Z",
     "start_time": "2024-12-11T16:22:43.967976Z"
    }
   },
   "source": [
    "# Комментарии с ютуба (\"https://www.youtube.com/watch?v=rGyQHyDMZZI\")\n",
    "reviews = [\n",
    "    'This guy got soul.',\n",
    "    \"Perfect blend of Jazz, Funk and City Pop. Edit: May I just add, those RNB/Slow Jams beats is a reminiscent of mid 2000's vibe. Thanks for the likes guys!\",\n",
    "    \"I haven’t really listened to this artist before, this made me download his every song\",\n",
    "    \"Just want to open this video for the background music, but I ended up watched and enjoyed a whole 29 minutes live performance. He sings with his voice, body, and soul. I immediately became a fan. = )\",\n",
    "    \"Just discovered this man now from this video and I am happy that I did. His music is so soothing and warming.\",\n",
    "    \"I used to watch his covers back in the day 😭 I loved his, “How Deep is Your Love,” and, “Plastic Love,” cover. So crazy he became so successful. So so much deserved\",\n",
    "    \"He finally graced NPR ... The world should know how amazing he is\",\n",
    "    \"Congratulation Fujii Kaze ...is the first time i see and hear about your music .. you are so great .. I am now your new fan from Lima Perù in Southamerica. kisses !!!!!\",\n",
    "    \"Wow. Just discovered this music yesterday. Shout out from San Antonio, Texas ❤️\",\n",
    "    \"I strongly petition to have this man perform at Coachella for 2025\",\n",
    "    \"Guys it's never too late to start listening to Fujii Kaze (typing error before)\",\n",
    "    \"his joy in music just overflows in this tiny desk... and i felt that! fan from Philippines here!\",\n",
    "    \"😢😢😢 i just know him today, SO GOOD!!!!!!\",\n",
    "    \"watching tiny desk from my tiny phone in my tiny room but the world suddenly feels endless🌟\",\n",
    "    \"Oh wow I’m hooked… just love his positive energy… his beautiful smile…awesome/amazing vocals…beautiful melody/ lyrics… the band….just refreshing 🙏🥰🇨🇦 merci ❤️🎼🎵\",\n",
    "    \"Omg 2024 is gonna be best year because i found this live🥹💞🌅\",\n",
    "    \"I LOVE FUJII KAZE!!! Ahhhhhh he’s such a talent 😭😭😩😩😭😭😭 Matsuri is one of my happy songs! Thank you Tiny Desk!!\",\n",
    "    \"I went to Fujii Kaze’s piano concert (yes, only him solo with his piano). I was wondering if I’ll get bored in over-stimulated world we live in today. Though I have been waiting for his concerts since mid-2021. Let me say: Him and his piano is one of the best concert I went to! He sing nicely, purely and could connect with all concert goers well❤\",\n",
    "    \"been a fan since his piano cover days on yt, got disconnected with the world for a while then one day saw his name on a famous tiktok song so i thought wait-- is this the same guy that i've been watching on yt before? and boom he's now blessing so much more people with his talent! i cant believe this guy's artistry i swear he's on a whole different level and im just glad many are starting to appreciate this really wonderful being named Fuji Kaze. <3\",\n",
    "    \"He needs to do a world tour 😭\",\n",
    "    \"Why is he singing so happily? He seems to express the joy of his soul in music with his whole body.\",\n",
    "    \"The groove, the energy! The translated CCs really took the way I appreciate his other songs wayyyy higher. Such beautiful lyrics held back by a language barrier, glad I watched this.\",\n",
    "    \"Unreal. First time listening to this amazing musician and I am now obsessed.\",\n",
    "    \"With a beautiful heart, lovely hands and the voice of an angel I only wish I could have found him sooner so I could love him longer...I'm a 70 year old American woman who has experienced decades of wonderful music and I have to say Fujii Kaze is by far the best I have ever heard and I fully expect when I die to hear his voice in my ears\",\n",
    "    \"If y’all don’t know, Yo-sea (guy with beanie) is also a great Japanese artist\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:24:25.211750Z",
     "start_time": "2024-12-11T16:23:50.918219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt_tab')\n",
    "# Список стоп-слов\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Функция очистки текста\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Приведение к нижнему регистру\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    words = word_tokenize(text)  # Токенизация\n",
    "    words = [word for word in words if word not in stop_words]  # Удаление стоп-слов\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Применяем очистку к отзывам\n",
    "clean_reviews = [preprocess_text(review) for review in reviews]"
   ],
   "id": "361085928bfe7a37",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aidar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:33:22.079225Z",
     "start_time": "2024-12-11T16:33:22.061163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf.fit_transform(clean_reviews)\n",
    "\n",
    "# Получение матрицы признаков\n",
    "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))"
   ],
   "id": "8a0ec55a6d200e00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2000s vibe thanks      2024  2024 gon  2024 gon na      2025       29  \\\n",
      "0            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "1            0.183652  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "2            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "3            0.000000  0.000000  0.000000     0.000000  0.000000  0.18634   \n",
      "4            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "5            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "6            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "7            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "8            0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "9            0.000000  0.000000  0.000000     0.000000  0.415749  0.00000   \n",
      "10           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "11           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "12           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "13           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "14           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "15           0.000000  0.268522  0.268522     0.268522  0.000000  0.00000   \n",
      "16           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "17           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "18           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "19           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "20           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "21           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "22           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "23           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "24           0.000000  0.000000  0.000000     0.000000  0.000000  0.00000   \n",
      "\n",
      "    29 minutes  29 minutes live        70   70 year  ...  year found live  \\\n",
      "0      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "1      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "2      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "3      0.18634          0.18634  0.000000  0.000000  ...         0.000000   \n",
      "4      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "5      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "6      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "7      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "8      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "9      0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "10     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "11     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "12     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "13     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "14     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "15     0.00000          0.00000  0.000000  0.000000  ...         0.268522   \n",
      "16     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "17     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "18     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "19     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "20     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "21     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "22     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "23     0.00000          0.00000  0.112549  0.112549  ...         0.000000   \n",
      "24     0.00000          0.00000  0.000000  0.000000  ...         0.000000   \n",
      "\n",
      "    year old  year old american       yes  yes solo  yes solo piano  \\\n",
      "0   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "1   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "2   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "3   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "4   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "5   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "6   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "7   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "8   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "9   0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "10  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "11  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "12  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "13  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "14  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "15  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "16  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "17  0.000000           0.000000  0.116157  0.116157        0.116157   \n",
      "18  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "19  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "20  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "21  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "22  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "23  0.112549           0.112549  0.000000  0.000000        0.000000   \n",
      "24  0.000000           0.000000  0.000000  0.000000        0.000000   \n",
      "\n",
      "    yesterday  yesterday shout  yesterday shout san        yt  \n",
      "0    0.000000         0.000000             0.000000  0.000000  \n",
      "1    0.000000         0.000000             0.000000  0.000000  \n",
      "2    0.000000         0.000000             0.000000  0.000000  \n",
      "3    0.000000         0.000000             0.000000  0.000000  \n",
      "4    0.000000         0.000000             0.000000  0.000000  \n",
      "5    0.000000         0.000000             0.000000  0.000000  \n",
      "6    0.000000         0.000000             0.000000  0.000000  \n",
      "7    0.000000         0.000000             0.000000  0.000000  \n",
      "8    0.302276         0.302276             0.302276  0.000000  \n",
      "9    0.000000         0.000000             0.000000  0.000000  \n",
      "10   0.000000         0.000000             0.000000  0.000000  \n",
      "11   0.000000         0.000000             0.000000  0.000000  \n",
      "12   0.000000         0.000000             0.000000  0.000000  \n",
      "13   0.000000         0.000000             0.000000  0.000000  \n",
      "14   0.000000         0.000000             0.000000  0.000000  \n",
      "15   0.000000         0.000000             0.000000  0.000000  \n",
      "16   0.000000         0.000000             0.000000  0.000000  \n",
      "17   0.000000         0.000000             0.000000  0.000000  \n",
      "18   0.000000         0.000000             0.000000  0.222076  \n",
      "19   0.000000         0.000000             0.000000  0.000000  \n",
      "20   0.000000         0.000000             0.000000  0.000000  \n",
      "21   0.000000         0.000000             0.000000  0.000000  \n",
      "22   0.000000         0.000000             0.000000  0.000000  \n",
      "23   0.000000         0.000000             0.000000  0.000000  \n",
      "24   0.000000         0.000000             0.000000  0.000000  \n",
      "\n",
      "[25 rows x 500 columns]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:33:24.504220Z",
     "start_time": "2024-12-11T16:33:24.490974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "X_count = count_vectorizer.fit_transform(clean_reviews)\n",
    "\n",
    "# Печать топ-10 признаков\n",
    "print(pd.DataFrame(X_count.toarray(), columns=count_vectorizer.get_feature_names_out()).head(10))"
   ],
   "id": "c9f21e29b8a3c9d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2000s  2000s vibe  2000s vibe thanks  2024  2024 gon  2024 gon na  2025  \\\n",
      "0      0           0                  0     0         0            0     0   \n",
      "1      1           1                  1     0         0            0     0   \n",
      "2      0           0                  0     0         0            0     0   \n",
      "3      0           0                  0     0         0            0     0   \n",
      "4      0           0                  0     0         0            0     0   \n",
      "5      0           0                  0     0         0            0     0   \n",
      "6      0           0                  0     0         0            0     0   \n",
      "7      0           0                  0     0         0            0     0   \n",
      "8      0           0                  0     0         0            0     0   \n",
      "9      0           0                  0     0         0            0     1   \n",
      "\n",
      "   29  29 minutes  29 minutes live  ...  yesterday shout  yesterday shout san  \\\n",
      "0   0           0                0  ...                0                    0   \n",
      "1   0           0                0  ...                0                    0   \n",
      "2   0           0                0  ...                0                    0   \n",
      "3   1           1                1  ...                0                    0   \n",
      "4   0           0                0  ...                0                    0   \n",
      "5   0           0                0  ...                0                    0   \n",
      "6   0           0                0  ...                0                    0   \n",
      "7   0           0                0  ...                0                    0   \n",
      "8   0           0                0  ...                1                    1   \n",
      "9   0           0                0  ...                0                    0   \n",
      "\n",
      "   yosea  yosea guy  yosea guy beanie  yt  yt boom  yt boom hes  yt got  \\\n",
      "0      0          0                 0   0        0            0       0   \n",
      "1      0          0                 0   0        0            0       0   \n",
      "2      0          0                 0   0        0            0       0   \n",
      "3      0          0                 0   0        0            0       0   \n",
      "4      0          0                 0   0        0            0       0   \n",
      "5      0          0                 0   0        0            0       0   \n",
      "6      0          0                 0   0        0            0       0   \n",
      "7      0          0                 0   0        0            0       0   \n",
      "8      0          0                 0   0        0            0       0   \n",
      "9      0          0                 0   0        0            0       0   \n",
      "\n",
      "   yt got disconnected  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "3                    0  \n",
      "4                    0  \n",
      "5                    0  \n",
      "6                    0  \n",
      "7                    0  \n",
      "8                    0  \n",
      "9                    0  \n",
      "\n",
      "[10 rows x 842 columns]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:33:26.384100Z",
     "start_time": "2024-12-11T16:33:26.370946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пример меток (1 — положительный отзыв, 0 — отрицательный)\n",
    "y = [1] * 12 + [0] * (len(clean_reviews) - 12)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "    ('regressor', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Обучение модели\n",
    "pipeline.fit(clean_reviews, y)\n",
    "\n",
    "# Предсказания\n",
    "predictions = pipeline.predict(clean_reviews)\n",
    "print(\"Predictions:\", predictions)"
   ],
   "id": "1d61e0a737379cb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:33:28.500256Z",
     "start_time": "2024-12-11T16:33:28.481192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Преобразуем текст в векторы\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(clean_reviews)\n",
    "\n",
    "# Кластеризация\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Предсказания кластеров\n",
    "clusters = kmeans.predict(X)\n",
    "print(\"Clusters:\", clusters)"
   ],
   "id": "936da8d9c3c7f04a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters: [1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T16:33:30.243805Z",
     "start_time": "2024-12-11T16:33:30.231015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Извлечение шагов пайплайна\n",
    "vectorizer = pipeline.named_steps['vectorizer']\n",
    "regressor = pipeline.named_steps['regressor']\n",
    "\n",
    "# Извлечение слов и коэффициентов\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = regressor.coef_[0]\n",
    "\n",
    "# Создание DataFrame для удобства анализа\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Сортировка по значимости\n",
    "coeff_df = coeff_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Топ-10 признаков для положительных отзывов\n",
    "print(\"Топ-10 признаков для положительных отзывов:\")\n",
    "print(coeff_df.head(10))\n",
    "\n",
    "# Топ-10 признаков для отрицательных отзывов\n",
    "print(\"\\nТоп-10 признаков для отрицательных отзывов:\")\n",
    "print(coeff_df.tail(10))"
   ],
   "id": "8643d96ae8a7a61d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 признаков для положительных отзывов:\n",
      "          Feature  Coefficient\n",
      "276      got soul     0.187879\n",
      "292  guy got soul     0.187879\n",
      "291       guy got     0.187879\n",
      "445           man     0.184854\n",
      "476         music     0.179508\n",
      "174    discovered     0.171505\n",
      "219           fan     0.141267\n",
      "738         video     0.139695\n",
      "273           got     0.136957\n",
      "657          soul     0.128922\n",
      "\n",
      "Топ-10 признаков для отрицательных отзывов:\n",
      "              Feature  Coefficient\n",
      "503       needs world    -0.165077\n",
      "814        world tour    -0.165077\n",
      "502             needs    -0.165077\n",
      "504  needs world tour    -0.165077\n",
      "723              tour    -0.165077\n",
      "383        know today    -0.165924\n",
      "384   know today good    -0.165924\n",
      "717        today good    -0.165924\n",
      "272              good    -0.165924\n",
      "716             today    -0.178620\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h2>Вывод</h2>\n",
    "<p>\n",
    "Создан набор из 24 отзывов. Тексты обработаны: удалены стоп-слова, приведены к нижнему регистру. Применение TF-IDF выявило значимые слова.\n",
    "Логистическая регрессия успешно классифицировала отзывы на положительные и отрицательные. Использование n-грамм улучшило анализ.\n",
    "Ключевыми словами для классификации стали \"fan\", \"music\", \"soul\" для положительных отзывов и \"tour\", \"needs\" слова для отрицательных.\n",
    "Для улучшения модели стоит увеличить объем данных, добавить больше отрицательных примеров и протестировать другие алгоритмы.\n",
    "</p>\n",
    "<p>\n",
    "Итог: подход с TF-IDF и n-граммами оказался эффективным для анализа текста.\n",
    "</p>"
   ],
   "id": "f9bd7976598ec7ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
